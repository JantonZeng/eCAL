{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pdb\n",
    "import re, random\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxyGatherer(object):\n",
    "    \"\"\"\n",
    "    Gathering data from http://www.xicidaili.com\n",
    "    \"\"\"\n",
    "    def __init__(self, hdr):\n",
    "        self.hdr = hdr\n",
    "    def getproxies(self):\n",
    "        \"\"\"\n",
    "        This function serves to gather all the proxies from the below url, whether they are elite or transparent.\n",
    "        \"\"\"\n",
    "        url = 'http://www.xicidaili.com'\n",
    "        proxies = []\n",
    "        ##-------- Beautiful Soup Extract------------##\n",
    "        req = urllib.request.Request(url, headers=self.hdr)\n",
    "        doc = urllib.request.urlopen(req).read()\n",
    "        soup = bs(doc, 'lxml')\n",
    "        list1 = [tr.find_all('td') for tr in soup.find_all('tr')]\n",
    "        for td_index in range(2,len(list1)):\n",
    "            td = list1[td_index]\n",
    "            if td:\n",
    "                ip_joints = td[1].text+\":\"+td[2].text\n",
    "                if td[5].text == 'HTTP':\n",
    "                    proxies.append({\"http\":ip_joints.encode('utf-8')})\n",
    "                elif td[5].text == 'HTTPS':\n",
    "                    proxies.append({\"https\":ip_joints.encode('utf-8')})\n",
    "\n",
    "#         list1 = [tr.find_all('td') for tr in soup.find_all('tr')]\n",
    "#         IP_list = [element[1].text for element in list1 if element]\n",
    "#         Port_list = [element[2].text for element in list1 if element]\n",
    "        \n",
    "        ##-------- Reg Ex Extract------------##\n",
    "#         IP_ADDRESS_PATH = '//td[3]/text()'\n",
    "#         PORT_ADDRESS_PATH = '//tr/td[3]/text()'\n",
    "#         try:\n",
    "#             req = urllib.request.Request(url, headers=hdr)\n",
    "#         except:\n",
    "#             print('Cannot load proxies info!')\n",
    "#         resp = urllib.request.urlopen(req)\n",
    "#         doc = resp.read().decode('utf-8')\n",
    "        \n",
    "        #-------- Pattern Matching------------#\n",
    "#         prep = re.compile(r\"\"\"<tr\\s.*>\\n.*<td\\s.*>.*</td>\\n.*<td>.*</td>\\n.*<td>.*</td>\\n.*<td>.*</td>\\n.*<td\\s.*>\n",
    "#         .*</td>\\n.*<td>.*</td>\\n.*<td>.*</td>\\n.*<td>.*</td>\\n.*</tr>\"\"\", re.VERBOSE)\n",
    "#         proxy_list = prep.findall(doc)  #Create a list of proxies, both transparent and highly autonomous\n",
    "#         proxy_list = list(set(proxy_list))  #Remove duplicate\n",
    "#         if not proxy_list:\n",
    "#             print('There is nothing in the proxy list..')\n",
    "#         if proxy_list:\n",
    "#             print('Successfully extract Proxies HTML!')\n",
    "#             return proxy_list\n",
    "        #-------- Pattern Matching------------#\n",
    "    \n",
    "#         IP_list = re.findall(IP_ADDRESS_PATH, doc)\n",
    "#         IP_list = list(set(re.findall(IP_ADDRESS_PATH, doc)))\n",
    "        ##-------- Reg Ex Extract------------##\n",
    "    \n",
    "        \n",
    "#         if not IP_list:\n",
    "#             print('There is nothing in the IP list..')\n",
    "#         if IP_list:\n",
    "#             print('Successfully extract IP!')\n",
    "#             return IP_list\n",
    "        return proxies\n",
    "    def proxies_cleanup(self, proxies, timeout_sec=(3.05,27)):\n",
    "        text_url = \"https://www.teld.cn\"\n",
    "        temp=len(proxies)\n",
    "        socket.setdefaulttimeout(3)\n",
    "        for proxy in proxies:\n",
    "            try:\n",
    "#                 req = urllib.request.Request(text_url, headers=self.hdr)\n",
    "#                 doc = urllib.request.urlopen(req).read()\n",
    "                req = requests.get(text_url, proxies=proxy, headers=self.hdr, timeout=timeout_sec)\n",
    "#                 pdb.set_trace()\n",
    "            except Exception, e:\n",
    "#                 print proxy\n",
    "#                 print e\n",
    "                proxies.remove(proxy)\n",
    "                continue\n",
    "        print(\"Got rid of \" + str((temp - len(proxies))) + \" unusable proxies.\")\n",
    "        return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
